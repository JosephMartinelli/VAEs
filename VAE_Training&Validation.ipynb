{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "class Linear(torch.nn.Module):\n",
        "    def __init__(self, in_features=None, output_features=None):\n",
        "        super(Linear, self).__init__()\n",
        "        self.output_features = output_features\n",
        "        self.in_features = in_features\n",
        "        self.W = torch.nn.Parameter(data=torch.rand(size=(self.in_features, output_features)).cuda())\n",
        "        torch.nn.init.xavier_normal(self.W)\n",
        "        # if in_features is None or output_features is None:\n",
        "        #     self.W = None\n",
        "        # else:\n",
        "        #     self.W = torch.nn.Parameter(data=torch.rand(size=(self.in_features, output_features)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.matmul(x.reshape(x.shape[0], x.shape[1],-1), self.W)  # + self.b\n",
        "\n",
        "    def __repr__(self):\n",
        "        return \"Linear(in_features = \" + str(self.in_features) + \" out_features = \" + str(self.output_features) + \")\"\n",
        "\n",
        "\n",
        "class Relu(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        # Chiamata alla superclasse\n",
        "        super(Relu, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.nn.functional.relu(x)\n",
        "\n",
        "class BatchNorm1D(torch.nn.BatchNorm1d):\n",
        "  def __init__(self, num_features, eps=1e-5, momentum=0.1,\n",
        "                affine=True, track_running_stats=True):\n",
        "      super(BatchNorm1D, self).__init__(\n",
        "          num_features, eps, momentum, affine, track_running_stats)\n",
        "      self.stability = 1e-05\n",
        "      # Scale and shifting parameters\n",
        "      self.gamma = torch.ones(num_features, requires_grad=True)\n",
        "      self.beta = torch.zeros(num_features, requires_grad=True)\n",
        "      self.eps = eps\n",
        "      self.momentum = momentum\n",
        "\n",
        "  def forward(self, x):\n",
        "      # calculate running estimates\n",
        "      if self.training:\n",
        "          mean = x.mean([0, 2, 3])\n",
        "          # use biased var in train\n",
        "          var = x.var([0, 2, 3], unbiased=False)\n",
        "          n = x.numel() / x.size(1)\n",
        "          with torch.no_grad():\n",
        "              self.running_mean = self.momentum * mean + (1 - self.momentum) * self.running_mean\n",
        "              # update running_var with unbiased var\n",
        "              self.running_var = self.momentum * var * n / (n - 1)\\\n",
        "                  + (1 - self.momentum) * self.running_var\n",
        "      else:\n",
        "          mean = self.running_mean\n",
        "          var = self.running_var\n",
        "\n",
        "      # Scale and shift\n",
        "      x = (x - mean[None, :, None, None]) / (torch.sqrt(var[None, :, None, None] + self.eps))\n",
        "      return x * self.gamma[None,:,None,None] + self.beta[None, :, None, None]"
      ],
      "metadata": {
        "id": "rhnDk7FJrF7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Fw7Ypp89Xbqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bf502bc-4a6d-4833-d4b3-5ac9a44f7a90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "class VAE(torch.nn.Module):\n",
        "    def __init__(self,input_shape,channels,h_dim,z_dim):\n",
        "        super(VAE,self).__init__()\n",
        "        #Encoder, image to hidden space\n",
        "        self.encode_input_batch = BatchNorm1D(channels,eps=1e-3).cuda()\n",
        "        self.img_2hid = Linear(input_shape,h_dim).cuda()\n",
        "        self.fc_mu,self.fc_std = Linear(h_dim,z_dim).cuda(),Linear(h_dim,z_dim).cuda()\n",
        "        self.norm_mu,self.norm_std = BatchNorm1D(channels,eps=1e-3).cuda(), BatchNorm1D(channels,eps=1e-3).cuda()\n",
        "        # The decoder from the z space goes back to the hidden\n",
        "        self.z_2hidden = Linear(z_dim,h_dim).cuda()\n",
        "        self.hidden_batch_norm = BatchNorm1D(channels,eps=1e-3).cuda()\n",
        "        # Finally from the hidden we go back to the input space\n",
        "        self.hidden_2img = Linear(h_dim,input_shape).cuda()\n",
        "        self.img_batch = BatchNorm1D(channels,eps=1e-3).cuda()\n",
        "        self.relu = Relu().cuda()\n",
        "        self.rloss = torch.nn.MSELoss(reduction=\"sum\").cuda()\n",
        "\n",
        "\n",
        "\n",
        "    def encode(self,x):\n",
        "        x = self.img_2hid(x) #+ 1e-6)\n",
        "        #print(\"To hidden: \",x)\n",
        "        #Normalize\n",
        "        x = self.encode_input_batch(x)\n",
        "        x = self.relu(x)\n",
        "        #print(\"Relu of hidden: \",x)\n",
        "        mu,sigma = self.fc_mu(x),self.fc_std(x)\n",
        "        mu,sigma = self.norm_mu(mu),self.norm_std(sigma)\n",
        "        return self.relu(mu),self.relu(sigma)\n",
        "\n",
        "    def decode(self,x):\n",
        "        first_hidden = self.z_2hidden(x)\n",
        "        hidden_norm = self.hidden_batch_norm(first_hidden)\n",
        "        leaky_relu = torch.nn.functional.leaky_relu(hidden_norm)\n",
        "        hidden_2img_space = self.hidden_2img(leaky_relu)\n",
        "        img_batch = self.img_batch(hidden_2img_space)\n",
        "        last_relu = self.relu(img_batch)\n",
        "        if torch.all(torch.isnan(last_relu)):\n",
        "            torch.save({\n",
        "                'x_shape': x.shape,\n",
        "                'first_hidden':first_hidden,\n",
        "                'hidden_norm':hidden_norm,\n",
        "                'leaky_relu':leaky_relu,\n",
        "                'hidden_2img_space':hidden_2img_space,\n",
        "                'img_batch':img_batch,\n",
        "                'last_relu':last_relu,\n",
        "            },'drive/MyDrive/ColabVAE/errorLog/decode_error.pt')\n",
        "            raise OverflowError\n",
        "        return last_relu\n",
        "\n",
        "    def normalize(self,x):\n",
        "        original = x\n",
        "        v_min, v_max = x.min(), x.max()\n",
        "        new_min, new_max = torch.Tensor([0]).float().to('cuda:0'), torch.Tensor([1]).float().to('cuda:0')\n",
        "        x = torch.add(torch.mul(torch.div(torch.sub(x, v_min), torch.sub(v_max, v_min)), torch.sub(new_max, new_min)),new_min).to('cuda:0')\n",
        "        if torch.all(torch.isnan(x)):\n",
        "                torch.save({\n",
        "                    'original': original,\n",
        "                    'min': v_min,\n",
        "                    'max': v_max,\n",
        "                    'x': x,\n",
        "                }, 'drive/MyDrive/ColabVAE/errorLog/normalization_error.pt')\n",
        "                raise OverflowError\n",
        "        return x\n",
        "\n",
        "\n",
        "    def forward(self,x):\n",
        "        original = x.shape\n",
        "        mu, sigma = self.encode(x)\n",
        "        normal = torch.distributions.Normal(0,1)\n",
        "        # Sample from latent distribution from encoder std = torch.exp(torch.log(sigma) / 2 )\n",
        "        epsilon = torch.randn_like(sigma)\n",
        "        z = mu + sigma.exp() * epsilon\n",
        "        if torch.all(torch.isnan(z)):\n",
        "                torch.save({\n",
        "                    'mu': mu,\n",
        "                    'sigma': sigma,\n",
        "                    'epsilon': epsilon,\n",
        "                    'z': z,\n",
        "                }, 'drive/MyDrive/ColabVAE/errorLog/forward_error.pt')\n",
        "                raise OverflowError\n",
        "        x_hat = self.decode(z)\n",
        "        x_hat = self.normalize(x_hat).reshape(original)\n",
        "        log_q = torch.sum(normal.log_prob(epsilon) - sigma)\n",
        "        recon_error = self.rloss(x_hat,x)\n",
        "        elbo = recon_error - log_q\n",
        "        return elbo,recon_error,x_hat"
      ],
      "metadata": {
        "id": "x9kHCPLwq5gs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqAVxFJNqSWw"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "def build_hyper_params():\n",
        "    epochs = [15]\n",
        "    batch = [1000]\n",
        "    hidden_features = [400,600,800]\n",
        "    z_features = [100,200,400]\n",
        "    lr = [1,1e-5,3e-2]\n",
        "    grad_clip = [1.0,2e-4]\n",
        "    hyper_list = [epochs,batch,hidden_features,z_features,lr,grad_clip]\n",
        "    return list(itertools.product(*hyper_list))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "\n",
        "def load_data(batch,data_dir=\"./data\"):\n",
        "\n",
        "    trainset = torchvision.datasets.MNIST(\n",
        "        root=data_dir, train=True, download=True,transform=transforms.ToTensor())\n",
        "\n",
        "    testset = torchvision.datasets.MNIST(\n",
        "        root=data_dir, train=False, download=True,transform=transforms.ToTensor())\n",
        "\n",
        "\n",
        "    # Partitioning the dataset in 80% train & 20% validation\n",
        "    test_abs = int(len(trainset) * 0.8)\n",
        "    train_subset, val_subset = random_split(trainset, [test_abs, len(trainset) - test_abs])\n",
        "    trainloader = torch.utils.data.DataLoader(\n",
        "        train_subset, batch_size=batch, shuffle=True, num_workers=1\n",
        "    )\n",
        "\n",
        "    valloader = torch.utils.data.DataLoader(\n",
        "        val_subset, batch_size=batch , shuffle=True, num_workers=1\n",
        "    )\n",
        "\n",
        "    testloader = torch.utils.data.DataLoader(\n",
        "        testset, batch_size=batch, shuffle=False, num_workers=1\n",
        "    )\n",
        "\n",
        "    return trainloader,valloader,testloader"
      ],
      "metadata": {
        "id": "MgP1sCk4r-eE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "########## MAIN LOOP #############################\n",
        "\n",
        "configurations = build_hyper_params()\n",
        "configurations = configurations[configurations.index((15, 1000, 400, 400, 0.03, 1.0)) + 1:configurations.index((15, 1000, 600, 400, 1, 1.0))]\n",
        "loop = tqdm(configurations)\n",
        "print(\"Model selection on \",len(configurations),\" models\")\n",
        "for i,config in enumerate(loop):\n",
        "    tr_loss_list,val_loss_list = [],[]\n",
        "    tr_rec_list, val_rec_list = [], []\n",
        "    epochs, batch, hidden_features, z_features, lr, grad_clip = config\n",
        "    net = VAE(28*28,1,hidden_features,z_features).cuda()\n",
        "    opt = torch.optim.Adam(net.parameters(),lr)\n",
        "    #Load data\n",
        "    trainset,valset,_ = load_data(batch)\n",
        "    tr_loss,val_loss = 0,0\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Initial checkpoint\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'config': config,\n",
        "            'model_state_dict': net.state_dict(),\n",
        "            'optimizer_state_dict': opt.state_dict(),\n",
        "            'loss_list': tr_loss_list,\n",
        "            'val_loss_list':val_loss_list,\n",
        "            'tr_rec_list': tr_rec_list,\n",
        "            'val_rec_list': val_rec_list\n",
        "        },'drive/MyDrive/ColabVAE/checkpoints/before_training.pt')\n",
        "\n",
        "        # Train loop\n",
        "        net.train()\n",
        "        loop.set_postfix(Trying=config, Status='Training',Epoch=epoch)\n",
        "        for i,data in enumerate(trainset):\n",
        "                opt.zero_grad()\n",
        "                x,labels = data\n",
        "                x = x.to('cuda:0')\n",
        "                #t0 = time.time()\n",
        "                tr_loss,tr_rec,_ = net(x)\n",
        "                #print(\"Forward took: \",str(time.time() - t0))\n",
        "                tr_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(net.parameters(),lr)\n",
        "                opt.step()\n",
        "\n",
        "        #Validate\n",
        "        net.eval()\n",
        "        loop.set_postfix(Trying=config, Status='Validating',Epoch=epoch)\n",
        "        for i,data in enumerate(valset):\n",
        "            with torch.no_grad():\n",
        "                x, labels = data\n",
        "                x = x.to('cuda:0')\n",
        "                val_loss,val_rec,_ = net(x)\n",
        "\n",
        "        tr_loss_list.append(tr_loss),val_loss_list.append(val_loss)\n",
        "        tr_rec_list.append(tr_rec), val_rec_list.append(val_rec)\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'config': config,\n",
        "            'model_state_dict': net.state_dict(),\n",
        "            'optimizer_state_dict': opt.state_dict(),\n",
        "            'loss_list': tr_loss_list,\n",
        "            'val_loss_list':val_loss_list,\n",
        "            'tr_rec_list': tr_rec_list,\n",
        "            'val_rec_list': val_rec_list\n",
        "        },'drive/MyDrive/ColabVAE/checkpoints/after_val.pt')\n",
        "\n",
        "    # Save model & performance\n",
        "    torch.save(net,'drive/MyDrive/ColabVAE/models/model' + str(config) + '.pt')\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'config': config,\n",
        "        'model_state_dict': net.state_dict(),\n",
        "        'optimizer_state_dict': opt.state_dict(),\n",
        "        'loss_list': tr_loss_list,\n",
        "        'val_loss_list': val_loss_list,\n",
        "        'tr_rec_list': tr_rec_list,\n",
        "        'val_rec_list': val_rec_list\n",
        "    },'drive/MyDrive/ColabVAE/performance/model' + str(config) + 'perf.pt')\n",
        "\n"
      ],
      "metadata": {
        "id": "-Wb6SQNtr5qG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "041e6108-2b57-466c-a0e6-510c27e042c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-4b996683f5a1>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    configurations = build_hyper_params()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3-kkBrkiHL8",
        "outputId": "9fd2ebaf-7fa4-4dcd-d789-c70fbb5f4785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model  0  Loss:  195899.125  Validation:  215140.984375  ReconLoss:  54371.19921875  ReconVal:  72968.796875\n",
            "Model  1  Loss:  220538.359375  Validation:  215072.71875  ReconLoss:  78426.640625  ReconVal:  73285.09375\n",
            "Model  2  Loss:  263671.875  Validation:  265030.53125  ReconLoss:  83039.203125  ReconVal:  85131.7890625\n",
            "Model  3  Loss:  264015.8125  Validation:  262635.25  ReconLoss:  83062.796875  ReconVal:  82671.796875\n",
            "Model  4  Loss:  190098.09375  Validation:  189923.53125  ReconLoss:  47923.69921875  ReconVal:  48096.3984375\n",
            "Model  5  Loss:  190142.15625  Validation:  190219.8125  ReconLoss:  48408.3515625  ReconVal:  48268.77734375\n",
            "Model  6  Loss:  330854.5  Validation:  332560.3125  ReconLoss:  47244.5625  ReconVal:  48438.671875\n",
            "Model  7  Loss:  336466.875  Validation:  337272.59375  ReconLoss:  52819.56640625  ReconVal:  54076.36328125\n",
            "Model  8  Loss:  445759.125  Validation:  444013.96875  ReconLoss:  85258.34375  ReconVal:  84896.9453125\n",
            "Model  9  Loss:  445676.1875  Validation:  444897.25  ReconLoss:  83784.203125  ReconVal:  84785.546875\n",
            "Model  10  Loss:  334117.84375  Validation:  333203.15625  ReconLoss:  50177.4296875  ReconVal:  49989.1015625\n",
            "Model  11  Loss:  333711.0625  Validation:  334200.8125  ReconLoss:  49932.7421875  ReconVal:  50732.1015625\n",
            "Model  12  Loss:  623019.75  Validation:  621137.0  ReconLoss:  54466.51171875  ReconVal:  54009.0546875\n"
          ]
        }
      ]
    }
  ]
}